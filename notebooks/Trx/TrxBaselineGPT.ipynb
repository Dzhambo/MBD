{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f7fe90-2d0c-4cce-853a-079ffb802c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ptls.data_load import IterableChain\n",
    "from ptls.data_load.iterable_processing.iterable_seq_len_limit import ISeqLenLimit\n",
    "from ptls.data_load.iterable_processing.category_size_clip import CategorySizeClip\n",
    "from ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\n",
    "from ptls.data_load.iterable_processing.target_move import TargetMove\n",
    "from ptls.data_load.datasets.parquet_dataset import ParquetDataset, ParquetFiles\n",
    "from ptls.data_load.iterable_processing.feature_filter import FeatureFilter\n",
    "\n",
    "from functools import partial\n",
    "from ptls.nn import TrxEncoder, RnnSeqEncoder\n",
    "from ptls.frames.coles import CoLESModule\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter\n",
    "from ptls.data_load.iterable_processing import FeatureBinScaler\n",
    "from ptls.frames.coles import ColesIterableDataset\n",
    "from ptls.frames.coles.split_strategy import SampleSlices\n",
    "from ptls.frames import PtlsDataModule\n",
    "from ptls.data_load.datasets import inference_data_loader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "from itertools import groupby\n",
    "from functools import reduce\n",
    "from operator import iadd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from ptls.data_load.feature_dict import FeatureDict\n",
    "from ptls.frames.coles.split_strategy import AbsSplit\n",
    "\n",
    "from functools import partial\n",
    "from ptls.nn import TrxEncoder\n",
    "from ptls.nn.seq_encoder.rnn_encoder import RnnEncoder\n",
    "from ptls.frames.coles.coles_module import CoLESModule\n",
    "from ptls.frames.inference_module import InferenceModule\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from ptls.data_load.utils import collate_feature_dict\n",
    "from tqdm.auto import tqdm\n",
    "import lightgbm as ltb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af38a1fc-763c-4c6b-bcf1-ff739b9862f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ptls.frames.bert import  MlmDataset, MlmIterableDataset\n",
    "from ptls.frames.tabformer.tabformer_dataset import TabformerDataset,  TabformerIterableDataset\n",
    "from ptls.frames.gpt.gpt_dataset import GptDataset,  GptIterableDataset\n",
    "from ptls.nn import TabFormerFeatureEncoder, TransformerEncoder, RnnEncoder\n",
    "from ptls.frames.tabformer.tabformer_module import TabformerPretrainModule\n",
    "from ptls.frames.gpt.gpt_module import GptPretrainModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af005486-1ad4-466e-9119-935c91ffea2e",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7d5a89-9480-4b6f-bfd0-7414e25d6e87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_path = 'trx_train_prepr.parquet'\n",
    "valid_data_path = 'trx_test_prepr.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92bfd792-7678-48d4-87ea-a0d9b77701af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_conf = {\n",
    "    'category_max_size': { 'amount': 7,\n",
    "                          'event_type': 57, \n",
    "                          'event_subtype': 58, \n",
    "                          'src_type11': 84, \n",
    "                          'src_type12': 348, \n",
    "                          'dst_type11': 83, \n",
    "                          'dst_type12': 416, \n",
    "                          'src_type22': 89,\n",
    "                          'src_type32': 90,\n",
    "                         },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "387d4b8c-31be-4825-bf0d-8313500841ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process = IterableChain(\n",
    "            SeqLenFilter(min_seq_len=32),\n",
    "            ISeqLenLimit(max_seq_len=4096),\n",
    "            FeatureFilter(drop_feature_names=['client_id', 'target_1', 'target_2', 'target_3', 'target_4']),\n",
    "            FeatureBinScaler({'amount': [0.1783, 0.2350, 0.2589, 0.3690, 0.5132, 0.5557, 0.6401]}),\n",
    "            CategorySizeClip(dataset_conf['category_max_size']),\n",
    "            ToTorch()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cbc69d-ce3a-4dfe-8bf7-0b0023250b78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = ParquetDataset([train_data_path], post_processing=process, shuffle_files=True)\n",
    "valid = ParquetDataset([valid_data_path], post_processing=process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18eacdd9-e49d-4d84-a096-3463857c4056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = GptIterableDataset(\n",
    "    data=train,\n",
    "    min_len=50,\n",
    "    max_len=300\n",
    ")\n",
    "\n",
    "valid_ds = MlmIterableDataset(\n",
    "    data=train,\n",
    "    min_len=50,\n",
    "    max_len=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa3566e5-fe44-4b53-8075-3d04f7a9ca64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dl = PtlsDataModule(\n",
    "    train_data=train_ds,\n",
    "    train_num_workers=16,\n",
    "    train_batch_size=256,\n",
    "    valid_data=valid_ds,\n",
    "    valid_num_workers=16,\n",
    "    valid_batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d99af1-02ec-4dc7-9672-742c03b65696",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66ecf4a6-6432-4c34-ad68-b652e3b652a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trx_encoder_params = dict(\n",
    "    embeddings_noise=0.003, \n",
    "    embeddings={\n",
    "        'amount': {'in': 8, 'out': 24},\n",
    "        \"event_type\": {'in': 58, \"out\": 24},\n",
    "        \"event_subtype\": {'in': 59, \"out\": 24},\n",
    "        'src_type11': {'in': 85, 'out': 24},\n",
    "        'src_type12': {'in': 349, 'out': 24},\n",
    "        'dst_type11': {'in': 84, 'out': 24},\n",
    "        'dst_type12': {'in': 417, 'out': 24}, \n",
    "        'src_type22': {'in': 90, 'out': 24},\n",
    "        'src_type32': {'in': 91, 'out': 24},\n",
    "      }\n",
    ")\n",
    "\n",
    "trx_encoder=TrxEncoder(**trx_encoder_params)\n",
    "trx_encoder.numeric_values = False\n",
    "\n",
    "seq_encoder = RnnEncoder(\n",
    "    input_size=9*24,\n",
    "    hidden_size=256,\n",
    "    type='gru',\n",
    "    num_layers=1\n",
    ")\n",
    "\n",
    "model = GptPretrainModule(\n",
    "    trx_encoder=trx_encoder,\n",
    "    seq_encoder=seq_encoder,\n",
    "    total_steps=2477 * 15 + 10,\n",
    "    inference_pooling_strategy='out',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da41bbb-06e9-434d-904a-268f3916b5b3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b977403-28ef-46b5-b976-2e1a81da8329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import logging\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    limit_val_batches=5000,\n",
    "    gpus=[0],\n",
    "    enable_progress_bar=False,\n",
    "    gradient_clip_val=0.5,\n",
    "    logger=pl.loggers.TensorBoardLogger(\n",
    "        save_dir='./logdir',\n",
    "        name='baseline_gpt'\n",
    "    ),\n",
    "    callbacks=[\n",
    "        pl.callbacks.LearningRateMonitor(logging_interval='step'),\n",
    "        pl.callbacks.ModelCheckpoint(every_n_train_steps=5000, save_top_k=-1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef2fd4a-b538-4a1a-90ac-8486e1905fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cde3832-e5d9-419f-881b-3584cd0230c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.seq_encoder, '../models/trx_baseline_gpt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d736b044-cf88-4c96-b9aa-b3c7aaf50b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../models/trx_baseline_gpt.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df91ab4-b046-4503-8391-bc6378ada9b2",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29ee789b-cb32-4909-9090-7a2d87ceb855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ptls.data_load.iterable_processing_dataset import IterableProcessingDataset\n",
    "from datetime import datetime\n",
    "from ptls.data_load.padded_batch import PaddedBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7333db4-0930-4776-aa39-c47cdac276f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GetSplit(IterableProcessingDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_month,\n",
    "        end_month,\n",
    "        year=2022,\n",
    "        col_id='client_id',\n",
    "        col_time='event_time'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.start_month = start_month\n",
    "        self.end_month = end_month\n",
    "        self._year = year\n",
    "        self._col_id = col_id\n",
    "        self._col_time = col_time\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for rec in self._src:\n",
    "            for month in range(self.start_month, self.end_month+1):\n",
    "                features = rec[0] if type(rec) is tuple else rec\n",
    "                features = features.copy()\n",
    "                \n",
    "                if month == 12:\n",
    "                    month_event_time = datetime(self._year + 1, 1, 1).timestamp()\n",
    "                else:\n",
    "                    month_event_time = datetime(self._year, month + 1, 1).timestamp()\n",
    "                    \n",
    "                year_event_time = datetime(self._year, 1, 1).timestamp()\n",
    "                \n",
    "                mask = features[self._col_time] < month_event_time\n",
    "                \n",
    "                for key, tensor in features.items():\n",
    "                    if key.startswith('target'):\n",
    "                        features[key] = tensor[month - 1].tolist()    \n",
    "                    elif key != self._col_id:\n",
    "                        features[key] = tensor[mask] \n",
    "                            \n",
    "                features[self._col_id] += '_month=' + str(month)\n",
    "\n",
    "                yield features\n",
    "                \n",
    "def collate_feature_dict_with_target(batch, col_id='client_id', targets=False):\n",
    "    batch_ids = []\n",
    "    target_cols = []\n",
    "    for sample in batch:\n",
    "        batch_ids.append(sample[col_id])\n",
    "        del sample[col_id]\n",
    "        \n",
    "        if targets:\n",
    "            target_cols.append([sample[f'target_{i}'] for i in range(1, 5)])\n",
    "            del sample['target_1']\n",
    "            del sample['target_2']\n",
    "            del sample['target_3']\n",
    "            del sample['target_4']\n",
    "            \n",
    "    padded_batch = collate_feature_dict(batch)\n",
    "    if targets:\n",
    "        return padded_batch, batch_ids, target_cols\n",
    "    return padded_batch, batch_ids\n",
    "\n",
    "\n",
    "class InferenceModuleMultimodal(pl.LightningModule):\n",
    "    def __init__(self, model, pandas_output=True, drop_seq_features=True, model_out_name='out'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.pandas_output = pandas_output\n",
    "        self.drop_seq_features = drop_seq_features\n",
    "        self.model_out_name = model_out_name\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_len = len(x)\n",
    "        if x_len == 3:\n",
    "            x, batch_ids, target_cols = x\n",
    "        else: \n",
    "            x, batch_ids = x\n",
    "            \n",
    "        out = self.model(x)\n",
    "        if x_len == 3:\n",
    "            target_cols = torch.tensor(target_cols)\n",
    "            x_out = {\n",
    "                'client_id': batch_ids,\n",
    "                'target_1': target_cols[:, 0],\n",
    "                'target_2': target_cols[:, 1],\n",
    "                'target_3': target_cols[:, 2],\n",
    "                'target_4': target_cols[:, 3],\n",
    "                self.model_out_name: out\n",
    "            }\n",
    "        else:\n",
    "            x_out = {\n",
    "                'client_id': batch_ids,\n",
    "                self.model_out_name: out\n",
    "            }\n",
    "\n",
    "        if self.pandas_output:\n",
    "            return self.to_pandas(x_out)\n",
    "        return x_out\n",
    "\n",
    "    @staticmethod\n",
    "    def to_pandas(x):\n",
    "        expand_cols = []\n",
    "        scalar_features = {}\n",
    "\n",
    "        for k, v in x.items():\n",
    "            if type(v) is torch.Tensor:\n",
    "                v = v.cpu().numpy()\n",
    "\n",
    "            if type(v) is list or len(v.shape) == 1:\n",
    "                scalar_features[k] = v\n",
    "            elif len(v.shape) == 2:\n",
    "                expand_cols.append(k)\n",
    "            else:\n",
    "                scalar_features[k] = None\n",
    "\n",
    "        dataframes = [pd.DataFrame(scalar_features)]\n",
    "        for col in expand_cols:\n",
    "            v = x[col].cpu().numpy()\n",
    "            dataframes.append(pd.DataFrame(v, columns=[f'{col}_{i:04d}' for i in range(v.shape[1])]))\n",
    "\n",
    "        return pd.concat(dataframes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12dec37c-4975-494a-9425-a5d4f8d56626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_process = IterableChain(\n",
    "            FeatureFilter(keep_feature_names=['client_id', 'target_1', 'target_2', 'target_3', 'target_4']),\n",
    "            GetSplit(start_month=1, end_month=12),\n",
    "            ToTorch()\n",
    ")\n",
    "\n",
    "test_process = IterableChain(\n",
    "            FeatureFilter(keep_feature_names=['client_id'], drop_feature_names=['target_1', 'target_2', 'target_3', 'target_4']),\n",
    "            ToTorch()\n",
    ")\n",
    "\n",
    "\n",
    "train = ParquetDataset([train_data_path], post_processing=train_process)\n",
    "test = ParquetDataset([valid_data_path], post_processing=test_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6989271-0c43-4b64-8d28-1a10494a2233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_train_dl = DataLoader(\n",
    "        dataset=train,\n",
    "        collate_fn=partial(collate_feature_dict_with_target, targets=True),\n",
    "        shuffle=False,\n",
    "        num_workers=16,\n",
    "        batch_size=512,\n",
    "    )\n",
    "\n",
    "inference_test_dl = DataLoader(\n",
    "        dataset=test,\n",
    "        collate_fn=collate_feature_dict_with_target,\n",
    "        shuffle=False,\n",
    "        num_workers=16,\n",
    "        batch_size=512,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb97e8e1-daeb-46a2-940c-0d2ff013b3b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inf_module = InferenceModuleMultimodal(\n",
    "        model=model,\n",
    "        pandas_output=True,\n",
    "        drop_seq_features=True,\n",
    "        model_out_name='emb',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "220e5cfd-2e71-4f5c-a507-f6bc02785596",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=[0], max_epochs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcd052e-3a57-4226-95cb-357d65a04ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inf_test_embeddings = pd.concat(\n",
    "        trainer.predict(inf_module, inference_test_dl)\n",
    "    )\n",
    "inf_test_embeddings.to_parquet(\"trx_baseline_gpt_test.parquet\", index=False, engine=\"pyarrow\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d36b1eb-5a96-4ebf-9a61-15e1a2660d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del inf_test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0113d8a-b3b9-45a5-b57c-982b58b5e7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inf_train_embeddings = pd.concat(\n",
    "        trainer.predict(inf_module, inference_train_dl)\n",
    "    )\n",
    "inf_train_embeddings.to_parquet(\"trx_baseline_gpt_train.parquet\", index=False, engine=\"pyarrow\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db74cb9b-f2a5-480c-b96c-6cfe5f71ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "del inf_train_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
